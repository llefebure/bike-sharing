---
title: "Forecasting Availability in a Bike Sharing Network"
output: html_document
---

During my summer travels, I visited many cities around the world with bike sharing systems. As someone who enjoys biking as a means of transport, the concept really appeals to me. Such systems encourage physical activity and can relieve congestion on other public transit options like busses and trams. They also offer another solution to the [Last Mile Problem](http://en.wikipedia.org/wiki/Last_mile_(transportation)), which refers to the question of how to get people from a transportation hub to their final destination.

As an example, the terminus of the Caltrain in San Francisco is quite far from the Financial District, so a transfer to an alternative form of transportation is necessary for commuters destined for that area. I spent a summer making that exact commute. My solution to the problem involved taking my bike with me on the train, but there is limited space for that. For a commuter who does not want to commit to carrying their bike on the crowded train everyday, having the option of picking up a shared bike at the station to go the last mile is enticing.

Bike sharing systems are not without flaws. The most obvious problem is that with enough demand, all of the bikes at your preferred pickup station could be gone. Another (potentially more frustrating) issue is that your preferred dropoff station could be full. Bike sharing systems usually employ people to drive around and rebalance the stock at different stations.

## The Task

In this analysis, I experiment with using Poisson regression for predicting the net change in bike availability at each station in the [Bay Area Bike Share](http://www.bayareabikeshare.com/) system over the next hour. The parametric structure and assumptions of Poisson regression enable probabilistic predictions rather than simply point predictions.

This task is important for two reasons. Users want to know how likely it is that there will be a bike at their preferred pickup station, and rebalancers need to know how they should distribute bikes to optimally handle future demand.

```{r echo=FALSE, eval=FALSE}
setwd("~/Documents/Projects/BikeShare/Rmd")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(readr)
library(stringr)
library(knitr)
library(skellam)
library(ggplot2)
library(AER)
source("../R/helpers.R")
```

## Data

The Bay Area Bike Share program makes trip, station, weather, and station availability data publically available [here](http://www.bayareabikeshare.com/open-data). The trip data consists of individual records for all 525,944 trips made between 3/1/2014 and 8/31/2015 complete with information like origin and destination, duration, and start time.

### Building the Data Set

I use `dplyr` to aggregate this trip data and strip out relevant features for training and testing. This data munging problem has its own challenges, and the full code for the `getProcessedTripData` function can be found [here](http://github.com/llefebure/bike-sharing/blob/master/R/helpers.R). In particular, I infer the number of arrivals and departures for each station at every hour of every day and append weather and station availability features. The full data set consists of eighteen months. I hold out the final three months of data for testing.

```{r warning=FALSE}
full <- getProcessedTripData() %>%
  filter(!is.na(bikes_available))
train <- full %>% filter(year == "2014" | (year == "2015" & month < "06"))
test <- full %>% filter(year == "2015" & month >= "06")
kable(head(test %>% filter (weekday == "Weekend")))
```

## Model Building 

I am interested in predicting the net change in the number of bikes at each station over the next hour. The net change is simply the number of arrivals minus the number of departures, so there are multiple ways to approach this problem. I can predict the net change directly or make predictions for the number of arrivals and departures separately. I focus efforts on the latter using Poisson regression.

### Poisson Regression
A Poisson process is a random process in which events occur independently of one another and the time between events is exponentially distributed. Under this model, the number of events that occur within a fixed time period is Poisson distrubuted. These processes are often used to model systems such as calls arriving to a call center or customers arriving at a store.

The Poisson process is a natural fit for modeling a bike sharing network. If we consider arrivals and departures as independent Poisson processes, the number of arrivals and departures to a station over the next hour are each Poisson distributed. Under this model, we have to assume that events are independent of each other. This is not entirely justified because it is certainly possible to have couples or groups of people riding bikes together, but this method has had success in similar settings.

To impose this model structure on the problem, I use Poisson regression. Poisson regression is a generalized linear model where the response is assumed to be Poisson distributed. I fit separate models for the arrival and departure processes and combine them to predict the net change.

### Features

I use weekday, hour, and their interaction as categorical predictors and mean temperature and bikes available as quantitative predictors.

### Fitting

I fit the model independently on each station's data.

```{r}
fitPoisson <- function(st) {
  train.st <- filter(train, station_id == st)
  test.st <- filter(test, station_id == st)
  arr.fit <- glm(arrivals ~ weekday + hour + weekday:hour + `Mean TemperatureF` + bikes_available,
                 data = train.st, family = "poisson")
  dep.fit <- glm(departures ~ weekday + hour + weekday:hour + `Mean TemperatureF` + bikes_available,
                 data = train.st, family = "poisson")
  list(arr.fit = arr.fit,
       dep.fit = dep.fit,
       test = test.st)
}
```

### Combining Arrival and Departure Models to make Probabilistic Predictions

The difference of two independent Poisson distributions follows the [Skellam distribution](http://en.wikipedia.org/wiki/Skellam_distribution). If we assume that the arrival and departure processes to a given station are independent, then the Poisson arrival and departure models allow us in theory to predict an entire distribution for the net change rather than a single point. The following example shows how this can be done.

```{r}
pois.fit <- fitPoisson(69) # station_id 69 is SF Caltrain
```

After fitting the arrival and departure models, we can use them to make point predictions as before. These predictions act as parameters to the Skellam distribution, and this allows us to predict percentiles.

```{r}
dist.pred <- mutate(pois.fit$test, 
                    arr.param = predict(pois.fit$arr.fit, newdata = pois.fit$test, 
                                                 type = "response"),
                    dep.param = predict(pois.fit$dep.fit, newdata = pois.fit$test, 
                                        type = "response"),
                    q.25 = qskellam(.25, arr.param, dep.param),
                    q.50 = qskellam(.50, arr.param, dep.param),
                    q.75 = qskellam(.75, arr.param, dep.param))
```

In the table below, we can see a slice from the test set that includes the true net change and quartile predictions for the net change.

```{r}
kable(head(dist.pred %>% filter(weekday == "Weekday") %>% select(hour, weekday, net, q.25, q.50, q.75), 15))
```

### Evaluation

Measuring the success of these models is difficult because the error rate for each station will naturally be different because of different demand patterns. Some stations have more demand and thus higher variance. Despite this problem, I can still benchmark the model against simple weekday/hour average predictions.

```{r eval=FALSE}
mse.comp <- t(sapply(unique(full$station_id), function(st) {
  mod <- fitPoisson(st)

  group.avgs <- train %>% group_by(station_id, weekday, hour) %>% summarize(group_avg = mean(net))
  test.group.avg <- inner_join(mod$test, group.avgs)
  mse.avg <- mean((test.group.avg$net - test.group.avg$group_avg)**2)
  
  preds <- predict(mod$arr.fit, newdata = mod$test, type = "response") - predict(mod$dep.fit, newdata = mod$test, type = "response")
  mse.pois <- mean((mod$test$net - preds)**2)
  
  c(mse.avg, mse.pois)
}))

mse.comp <- data.frame(mse.comp)
mse.comp$col <- ifelse(mse.comp$X1 > mse.comp$X2, "Good", "Bad")
qplot(mse.comp[,1], mse.comp[,2], color = mse.comp$col)
mean(mse.comp$col[mse.comp$X1 > 2] == "Good", na.rm = T)
```

It appears that Poisson regression outperforms the group average predictions only slightly for the higher variance stations. There is a bigger problem though. The Poisson model fit is badly overdispersed meaning that the data is not accurately represented by a Poisson distribution. This means that the percentile estimates cannot be very accurate.

```{r}
dispersiontest(pois.fit$arr.fit)
dispersiontest(pois.fit$dep.fit)
```

## Conclusion

My focus in this analysis was to leverage the assumptions of Poisson regression to make probabilistic predictions that have more practical use than point predictions. In theory, this is a really great approach, and my analysis above shows that it can produce very useful looking predictions. However, a prediction is only as good as its accuracy, and the Poisson model is overdispersed. In the Poisson distribution, there is only one parameter that controls both the mean and variance, and overdispersion refers to variance being larger than one would expect given the model parameters.

There are two ways around this problem. I can add in some more elaborate features and hope that the resulting Poisson model is not overdispersed, or I can use Negative Binomial regression. NB regression is similar to Poisson regression, but it adds in an extra parameter to help tune the variance. I will experiment with this further.